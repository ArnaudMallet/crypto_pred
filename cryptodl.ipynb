{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This dataset contains information on historic trades for several cryptoassets, such as Bitcoin and Ethereum. Your challenge is to predict their future returns.\n\nAs historic cryptocurrency prices are not confidential this will be a forecasting competition using the time series API. Furthermore the public leaderboard targets are publicly available and are provided as part of the competition dataset. Expect to see many people submitting perfect submissions for fun. Accordingly, THE PUBLIC LEADERBOARD FOR THIS COMPETITION IS NOT MEANINGFUL and is only provided as a convenience for anyone who wants to test their code. The final private leaderboard will be determined using real market data gathered after the submission period closes.","metadata":{}},{"cell_type":"markdown","source":"train.csv - The training set\n\n- timestamp - A timestamp for the minute covered by the row.\n- Asset_ID - An ID code for the cryptoasset.\n- Count - The number of trades that took place this minute.\n- Open - The USD price at the beginning of the minute.\n- High - The highest USD price during the minute.\n- Low - The lowest USD price during the minute.\n- Close - The USD price at the end of the minute.\n- Volume - The number of cryptoasset u units traded during the minute.\n- VWAP - The volume-weighted average price for the minute.\n- Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.\n- Weight - Weight, defined by the competition hosts here\n- Asset_Name - Human readable Asset name.\n\nexample_test.csv - An example of the data that will be delivered by the time series API.\n\nexample_sample_submission.csv - An example of the data that will be delivered by the time series API. The data is just copied from train.csv.\n\nasset_details.csv - Provides the real name and of the cryptoasset for each Asset_ID and the weight each cryptoasset receives in the metric.\n\nsupplemental_train.csv - After the submission period is over this file's data will be replaced with cryptoasset prices from the submission period. In the Evaluation phase, the train, train supplement, and test set will be contiguous in time, apart from any missing data. The current copy, which is just filled approximately the right amount of data from train.csv is provided as a placeholder.\n\n    ðŸ“Œ There are 14 coins in the dataset\n\n    ðŸ“Œ There are 4 years in the [full] dataset\n","metadata":{}},{"cell_type":"markdown","source":"#  Import","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nimport time","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:06:01.660087Z","iopub.execute_input":"2021-11-24T16:06:01.660482Z","iopub.status.idle":"2021-11-24T16:06:03.682785Z","shell.execute_reply.started":"2021-11-24T16:06:01.660388Z","shell.execute_reply":"2021-11-24T16:06:03.681736Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# check if CUDA is available\nuse_cuda = torch.cuda.is_available()\nprint(use_cuda)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:06:03.684343Z","iopub.execute_input":"2021-11-24T16:06:03.684678Z","iopub.status.idle":"2021-11-24T16:06:03.690128Z","shell.execute_reply.started":"2021-11-24T16:06:03.684648Z","shell.execute_reply":"2021-11-24T16:06:03.689230Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\ntrain_csv = pd.read_csv(\"/kaggle/input/g-research-crypto-forecasting/train.csv\")\ntest_csv = pd.read_csv(\"/kaggle/input/g-research-crypto-forecasting/example_test.csv\")\nsamples_submission_csv = pd.read_csv(\"/kaggle/input/g-research-crypto-forecasting/example_sample_submission.csv\")\nasset_csv = pd.read_csv(\"/kaggle/input/g-research-crypto-forecasting/asset_details.csv\")\nsupplemental_train = pd.read_csv(\"/kaggle/input/g-research-crypto-forecasting/supplemental_train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:06:03.691445Z","iopub.execute_input":"2021-11-24T16:06:03.691795Z","iopub.status.idle":"2021-11-24T16:07:07.976127Z","shell.execute_reply.started":"2021-11-24T16:06:03.691768Z","shell.execute_reply":"2021-11-24T16:07:07.974812Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Remove NaN rows","metadata":{}},{"cell_type":"code","source":"train_csv = train_csv.dropna()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:07:07.978507Z","iopub.execute_input":"2021-11-24T16:07:07.978771Z","iopub.status.idle":"2021-11-24T16:07:10.063948Z","shell.execute_reply.started":"2021-11-24T16:07:07.978741Z","shell.execute_reply":"2021-11-24T16:07:10.063242Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_csv.iloc[1:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:07:10.065123Z","iopub.execute_input":"2021-11-24T16:07:10.065539Z","iopub.status.idle":"2021-11-24T16:07:10.089610Z","shell.execute_reply.started":"2021-11-24T16:07:10.065505Z","shell.execute_reply":"2021-11-24T16:07:10.088818Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"asset_csv","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:07:10.090765Z","iopub.execute_input":"2021-11-24T16:07:10.090974Z","iopub.status.idle":"2021-11-24T16:07:10.100758Z","shell.execute_reply.started":"2021-11-24T16:07:10.090949Z","shell.execute_reply":"2021-11-24T16:07:10.100042Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_csv_BitcoinDash = train_csv[train_csv.Asset_ID==0]","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:07:10.102032Z","iopub.execute_input":"2021-11-24T16:07:10.102291Z","iopub.status.idle":"2021-11-24T16:07:10.374231Z","shell.execute_reply.started":"2021-11-24T16:07:10.102260Z","shell.execute_reply":"2021-11-24T16:07:10.373193Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"len(train_csv_BitcoinDash)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:07:10.375482Z","iopub.execute_input":"2021-11-24T16:07:10.375722Z","iopub.status.idle":"2021-11-24T16:07:10.382216Z","shell.execute_reply.started":"2021-11-24T16:07:10.375693Z","shell.execute_reply":"2021-11-24T16:07:10.381369Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Target evolution visualization","metadata":{}},{"cell_type":"code","source":"def plot_df(df, x, y, title=\"\", xlabel='Date', ylabel='Value', dpi=100):\n    plt.figure(figsize=(16,5), dpi=dpi)\n    plt.plot(x, y, color='tab:red')\n    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n    plt.show()\n\nplot_df(train_csv_BitcoinDash, x=train_csv_BitcoinDash.timestamp, y=train_csv_BitcoinDash.Target, title='Taregt evo')","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:07:10.383705Z","iopub.execute_input":"2021-11-24T16:07:10.384214Z","iopub.status.idle":"2021-11-24T16:07:10.935105Z","shell.execute_reply.started":"2021-11-24T16:07:10.384153Z","shell.execute_reply":"2021-11-24T16:07:10.934228Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(5,3, figsize=(10,10))\ni = 0\nj = 0\nfor nb in asset_csv.Asset_ID:\n    temp = asset_csv.Asset_Name[nb]\n    train_csv_temp = train_csv[train_csv.Asset_ID==nb]\n    axs[i][j].plot(train_csv_temp.timestamp, train_csv_temp.Target)\n    axs[i][j].title.set_text(temp)\n    i = (i+1)%5\n    if i == 4:\n        j+=1\nfig.tight_layout(pad=1.0)        \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:12:14.489984Z","iopub.execute_input":"2021-11-24T16:12:14.490716Z","iopub.status.idle":"2021-11-24T16:12:21.763156Z","shell.execute_reply.started":"2021-11-24T16:12:14.490658Z","shell.execute_reply":"2021-11-24T16:12:21.762285Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"i =0\nn=(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)\n\nfor nb in n:\n    i = (i+1)%5\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T16:07:19.856903Z","iopub.status.idle":"2021-11-24T16:07:19.857250Z","shell.execute_reply.started":"2021-11-24T16:07:19.857065Z","shell.execute_reply":"2021-11-24T16:07:19.857082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Memory review","metadata":{}},{"cell_type":"code","source":"train_csv.info(memory_usage = \"deep\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-24T16:07:19.858472Z","iopub.status.idle":"2021-11-24T16:07:19.858797Z","shell.execute_reply.started":"2021-11-24T16:07:19.858632Z","shell.execute_reply":"2021-11-24T16:07:19.858648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Memory Size Reduction","metadata":{}},{"cell_type":"code","source":"for column in train_csv:\n    print(column)\n    if train_csv[column].dtype == 'float64':\n        train_csv[column]=pd.to_numeric(train_csv[column], downcast='float')\n    if train_csv[column].dtype == 'int64':\n        train_csv[column]=pd.to_numeric(train_csv[column], downcast='integer')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv.info(memory_usage = \"deep\")","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset description","metadata":{}},{"cell_type":"code","source":"dtf_description_train = train_csv.describe()\ndtf_description_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtf_description_test = test_csv.describe()\ndtf_description_test","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtf_description_sample = samples_submission_csv.describe()\ndtf_description_sample","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtf_description_asset = asset_csv.describe()\ndtf_description_asset","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtf_supplemental_train= supplemental_train.describe()\ndtf_supplemental_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Do not Run\n\ntrain_csv otherwise the kernel will die","metadata":{}},{"cell_type":"markdown","source":"# Number of entries that are null/void","metadata":{}},{"cell_type":"code","source":"def list_null(dataset, feature, per):\n    # Input\n    # dataset: the selected dataset\n    # feature: the name of the feature\n    # per: the percentage of row that are void vs the whole column\n    # output\n    # list the column with a void percentage higher than per\n    res = []\n    for f in feature:\n        #print(\"feature: {} number of missing value: {} \\n the lengt of the dataset is {} and the percentage of null value is {}\".format(f, train_csv[f].isna().sum(), len(dataset), train_csv[f].isna().sum()/len(dataset)))\n        if (dataset[f].isna().sum()/len(dataset)) > per:\n            res.append(f)\n        \n    return res\n\nlist_null(train_csv, train_csv.columns, 0.001)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distribution info","metadata":{}},{"cell_type":"code","source":"'''y = train_csv_nnan['Target']\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y, kde=False, fit=stats.johnsonsu)\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=stats.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=stats.lognorm)'''\n\n# Note: output is highly concentrated around 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datashowcase","metadata":{}},{"cell_type":"code","source":"train_csv.Target[0:9]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, sharex=True)\naxes[0, 0].plot(train_csv.value)\naxes[0,0].set_title('Original Series')\nplot_acf(train_csv.value, ax=axes[0, 1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset preparation","metadata":{}},{"cell_type":"code","source":"class TimeSeriesDataset (Dataset):\n    def __init__(self, X, y, seq_len=1):\n        self.X = X\n        self.y = y\n        self.seq_len = seq_len\n    \n    def __len__(self):\n        return self.X.__len__() - (self.seq_len-1)\n    \n    def __getitem__(self, index):\n        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"train_dataset = TimeSeriesDataset(X, target, seq_len=100)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 50, shuffle = False)\n\nfor i, d in enumerate(train_dataloader):\n    print(i, d[0].shape, d[1].shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the RNN\n\nNext, we define an RNN in PyTorch. We'll use `nn.RNN` to create an RNN layer, then we'll add a last, fully-connected layer to get the output size that we want. An RNN takes in a number of parameters:\n* **input_size** - the size of the input\n* **hidden_dim** - the number of features in the RNN output and in the hidden state\n* **n_layers** - the number of layers that make up the RNN, typically 1-3; greater than 1 means that you'll create a stacked RNN\n* **batch_first** - whether or not the input/output of the RNN will have the batch_size as the first dimension (batch_size, seq_length, hidden_dim)\n\nTake a look at the [RNN documentation](https://pytorch.org/docs/stable/nn.html#rnn) to read more about recurrent layers.","metadata":{}},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n        super(RNN, self).__init__()\n        \n        self.hidden_dim=hidden_dim\n\n        # define an RNN with specified parameters\n        # batch_first means that the first dim of the input and output will be the batch_size\n        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n        \n        # last, fully-connected layer\n        self.fc = nn.Linear(hidden_dim, output_size)\n\n    def forward(self, x, hidden):\n        # x (batch_size, seq_length, input_size)\n        # hidden (n_layers, batch_size, hidden_dim)\n        # r_out (batch_size, time_step, hidden_size)\n        batch_size = x.size(0)\n        \n        # get RNN outputs\n        r_out, hidden = self.rnn(x, hidden)\n        # shape output to be (batch_size*seq_length, hidden_dim)\n        r_out = r_out.view(-1, self.hidden_dim)  \n        \n        # get final output \n        output = self.fc(r_out)\n        \n        return output, hidden","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check the input and output dimensions\n\nAs a check that your model is working as expected, test out how it responds to input data.","metadata":{}},{"cell_type":"code","source":"# test that dimensions are as expected\ntest_rnn = RNN(input_size=1, output_size=1, hidden_dim=10, n_layers=2)\n\n# generate evenly spaced, test data pts\n''' time_steps = np.linspace(0, np.pi, seq_length)\ndata = np.sin(time_steps)\ndata.resize((seq_length, 1))'''\n\ntest_input = torch.Tensor(data).unsqueeze(0) # give it a batch_size of 1 as first dimension\nprint('Input size: ', test_input.size())\n\n# test out rnn sizes\ntest_out, test_h = test_rnn(test_input, None)\nprint('Output size: ', test_out.size())\nprint('Hidden state size: ', test_h.size())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the RNN","metadata":{}},{"cell_type":"code","source":"# decide on hyperparameters\ninput_size=1 \noutput_size=1\nhidden_dim=32\nn_layers=1\n\n# instantiate an RNN\nrnn = RNN(input_size, output_size, hidden_dim, n_layers)\nprint(rnn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss and Optimization\n\nThis is a regression problem: can we train an RNN to accurately predict the next data point, given a current data point?\n\n>* The data points are coordinate values, so to compare a predicted and ground_truth point, we'll use a regression loss: the mean squared error.\n* It's typical to use an Adam optimizer for recurrent models.","metadata":{}},{"cell_type":"code","source":"# MSE loss and Adam optimizer with a learning rate of 0.01\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=0.01) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining the training function\n\nThis function takes in an rnn, a number of steps to train for, and returns a trained rnn. This function is also responsible for displaying the loss and the predictions, every so often.\n\n#### Hidden State\n\nPay close attention to the hidden state, here:\n* Before looping over a batch of training data, the hidden state is initialized\n* After a new hidden state is generated by the rnn, we get the latest hidden state, and use that as input to the rnn for the following steps","metadata":{}},{"cell_type":"code","source":"# train the RNN\ndef train(rnn, n_steps, print_every):\n    \n    # initialize the hidden state\n    hidden = None      \n    \n    for batch_i, step in enumerate(range(n_steps)):\n        # defining the training data \n        time_steps = np.linspace(step * np.pi, (step+1)*np.pi, seq_length + 1)\n        data = np.sin(time_steps)\n        data.resize((seq_length + 1, 1)) # input_size=1\n\n        x = data[:-1]\n        y = data[1:]\n        \n        # convert data into Tensors\n        x_tensor = torch.Tensor(x).unsqueeze(0) # unsqueeze gives a 1, batch_size dimension\n        y_tensor = torch.Tensor(y)\n\n        # outputs from the rnn\n        prediction, hidden = rnn(x_tensor, hidden)\n\n        ## Representing Memory ##\n        # make a new variable for hidden and detach the hidden state from its history\n        # this way, we don't backpropagate through the entire history\n        hidden = hidden.data\n\n        # calculate the loss\n        loss = criterion(prediction, y_tensor)\n        # zero gradients\n        optimizer.zero_grad()\n        # perform backprop and update weights\n        loss.backward()\n        optimizer.step()\n\n        # display loss and predictions\n        if batch_i%print_every == 0:        \n            print('Loss: ', loss.item())\n            plt.plot(time_steps[1:], x, 'r.') # input\n            plt.plot(time_steps[1:], prediction.data.numpy().flatten(), 'b.') # predictions\n            plt.show()\n    \n    return rnn\n","metadata":{},"execution_count":null,"outputs":[]}]}